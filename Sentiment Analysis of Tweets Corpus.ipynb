{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets Corpus\n",
    "\n",
    "Using **sentiment analysis** for the task of predicting the polarity (positive, negative) of opinions expressed in tweets. The work presented here was based on and attempts to replicate that of the following [blogpost](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/), which contains over 1.5 million compiled tweets, annotated with the sentiment polarity: 0 for negative sentiment, and 1 for positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ronaldpeic/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import csv\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header line: ['\\ufeffItemID', 'Sentiment', 'SentimentSource', 'SentimentText']\n",
      "['1', '0', 'Sentiment140', '                     is so sad for my APL friend.............']\n",
      "Total number of rows: 1578614\n"
     ]
    }
   ],
   "source": [
    "with open('Sentiment Analysis Dataset.csv', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    print(\"Header line: %s\" % next(reader))\n",
    "    annotated_data = [r for r in reader]\n",
    "print(annotated_data[0])\n",
    "print(\"Total number of rows:\", len(annotated_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Each element in the array `annotated_data` is made up of the following fields:\n",
    "* Item ID\n",
    "* Sentiment (0 if negative, 1 if positive)\n",
    "* Where the author sourced the data from (the entire file is compiled from multiple sources)\n",
    "* Tweeted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)  # use a seed value to manage the randomisation\n",
    "random.shuffle(annotated_data)\n",
    "# annotated_data = annotated_data[:500000]\n",
    "annotated_data = annotated_data[:50000]  # using a reduced sample size due to hardware limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Spliting the data as follows:\n",
    "* Training set: 80%\n",
    "* Dev-test set: 10%\n",
    "* Test set: 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold1 40000 threshold2 45000\n",
      "[['240313', '0', 'Sentiment140', \"@Honey_ It's nasty.   No reports of flooding as yet. Multiple reports of bad hair and wet pants however.\"], ['618735', '0', 'Sentiment140', 'aww poor lacey  am nawt even in the call lol'], ['74547', '1', 'Sentiment140', '@Anniejunieee heyyy  i saw u on icarly ! you rocked  xoxoxo'], ['979620', '0', 'Sentiment140', \"@leeagoldstein LOL. That's awesome. I was promised that when mine was taken it was being put in a safe place. It was so thrown out. \"], ['1125777', '1', 'Sentiment140', 'Ok, time for bed! See you all tomorrow ']]\n"
     ]
    }
   ],
   "source": [
    "threshold1 = int(len(annotated_data) *8/10)\n",
    "threshold2 = int(threshold1 + len(annotated_data) *1/10)\n",
    "print(\"threshold1\", threshold1, \"threshold2\", threshold2)\n",
    "\n",
    "train = annotated_data[:threshold1]\n",
    "dev = annotated_data[threshold1:threshold2]\n",
    "test = annotated_data[threshold2:]\n",
    "print(dev[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the data is balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data with positive sentiment: 0.492275\n",
      "train data with negative sentiment: 0.507725\n",
      "dev-test data with positive sentiment: 0.5034\n",
      "dev-test data with negative sentiment: 0.4966\n",
      "test data with positive sentiment: 0.501\n",
      "test data with negative sentiment: 0.499\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# reduced sample data such that each element is a tuple of (tweets, sentiment) for easier processing; \n",
    "train_tweets = [(row[3],int(row[1])) for row in train]\n",
    "dev_tweets = [(row[3],int(row[1])) for row in dev]\n",
    "test_tweets = [(row[3],int(row[1])) for row in test]\n",
    "\n",
    "# list of tweets in train set separated by sentiment\n",
    "train_sntmt_pos = [t for (t,s) in train_tweets if s == 1]\n",
    "train_sntmt_neg = [t for (t,s) in train_tweets if s == 0]\n",
    "print(\"train data with positive sentiment:\", len(train_sntmt_pos)/len(train_tweets))\n",
    "print(\"train data with negative sentiment:\", len(train_sntmt_neg)/len(train_tweets))\n",
    "\n",
    "# list of tweets in dev-test set separated by sentiment\n",
    "dev_sntmt_pos = [t for (t,s) in dev_tweets if s == 1]\n",
    "dev_sntmt_neg = [t for (t,s) in dev_tweets if s == 0]\n",
    "print(\"dev-test data with positive sentiment:\", len(dev_sntmt_pos)/len(dev_tweets))\n",
    "print(\"dev-test data with negative sentiment:\", len(dev_sntmt_neg)/len(dev_tweets))\n",
    "\n",
    "# list of tweets in test set separated by sentiment\n",
    "test_sntmt_pos = [t for (t,s) in test_tweets if s == 1]\n",
    "test_sntmt_neg = [t for (t,s) in test_tweets if s == 0]\n",
    "print(\"test data with positive sentiment:\", len(test_sntmt_pos)/len(test_tweets))\n",
    "print(\"test data with negative sentiment:\", len(test_sntmt_neg)/len(test_tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of the training set vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set vocabulary is 63906\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "train_vocab = set([word for row in train_tweets for word in word_tokenize(row[0])])\n",
    "print(\"size of training set vocabulary is\", len(train_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most occuring words for each tweet sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most occuring words in tweets with postitive sentiment: [('!', 13312), ('@', 11929), ('.', 9563), ('I', 6649), (',', 6412)]\n",
      "most occuring words in tweets with negative sentiment: [('.', 11436), ('!', 9928), ('I', 9770), ('@', 8247), ('to', 8006)]\n"
     ]
    }
   ],
   "source": [
    "train_tweets_pos = Counter([word for tweet in train_sntmt_pos for word in word_tokenize(tweet)])\n",
    "train_tweets_neg = Counter([word for tweet in train_sntmt_neg for word in word_tokenize(tweet)])\n",
    "\n",
    "print(\"most occuring words in tweets with postitive sentiment:\", train_tweets_pos.most_common()[:5])\n",
    "print(\"most occuring words in tweets with negative sentiment:\", train_tweets_neg.most_common()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding for Naive Bayes in NLTK\n",
    "Apply a feature extractor using one-hot encoding on the entire vocabulary in the training set. Use the feature extractor to train a Naive Bayes classifier in NLTK and report the accuracy of the classifier using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has(.)': True,\n",
       " 'has(Testing)': True,\n",
       " 'has(This)': True,\n",
       " 'has(a)': True,\n",
       " 'has(is)': True,\n",
       " 'has(test)': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature extractor that uses one-hot encoding\n",
    "def nltk_one_hot_extr(words, wordlist=None):\n",
    "    result = dict()\n",
    "    for w in word_tokenize(words):\n",
    "        if wordlist and w in wordlist:\n",
    "            result['has(%s)' % w] = True\n",
    "        elif wordlist == None:\n",
    "            result['has(%s)' % w] = True\n",
    "#         result['has(%s)' % w] = w in wordlist if wordlist else True\n",
    "    return result\n",
    "\n",
    "nltk_one_hot_extr(\"Testing. This is a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'has(@)': True, 'has(shabooty)': True, 'has(:)': True, 'has(Taylor)': True, 'has(Swift)': True, 'has(is)': True, 'has(19)': True, 'has(.)': True, 'has(She)': True, \"has('s)\": True, 'has(already)': True, 'has(way)': True, 'has(behind)': True, 'has(on)': True, 'has(her)': True, 'has(career)': True, 'has(Portman)': True, 'has(did)': True, 'has(it)': True, 'has(better)': True, 'has(,)': True, 'has(this)': True, 'has(just)': True, 'has(makes)': True, 'has(me)': True, 'has(cringe)': True}, 0), ({'has(@)': True, 'has(raccoon9ta)': True, 'has(you)': True, 'has(can)': True, 'has(do)': True, 'has(it)': True}, 1), ({'has(@)': True, 'has(TheStitchWitch)': True, 'has(Not)': True, 'has(on)': True, 'has(a)': True, 'has(friday)': True, 'has(!)': True, 'has(Get)': True, 'has(out)': True, 'has(the)': True, 'has(excedrin)': True, 'has(and)': True, 'has(caffeine)': True, 'has(show)': True, 'has(that)': True, 'has(b1+ch)': True, 'has(who)': True, \"has('s)\": True, 'has(boss)': True}, 1), ({'has(@)': True, 'has(LauraKim123)': True, 'has(yes)': True, 'has(,)': True, 'has(sorry)': True, 'has(!)': True, 'has(Saturday)': True, 'has(...)': True, 'has(haha)': True, 'has(was)': True, 'has(obviously)': True, 'has(very)': True, 'has(hopeful)': True, 'has(about)': True, 'has(the)': True, 'has(number)': True, 'has(of)': True, 'has(days)': True, 'has(left)': True, 'has(in)': True, 'has(this)': True, 'has(week)': True}, 0), ({'has(cause)': True, 'has(mom)': True, 'has(got)': True, 'has(up)': True, 'has(late)': True}, 0), ({'has(@)': True, 'has(krystlerb)': True, 'has(Leave)': True, 'has(it)': True, 'has(to)': True, 'has(u)': True, 'has(crack)': True, 'has(me)': True, 'has(up)': True, 'has(!)': True, 'has(IS)': True, 'has(that)': True, 'has(a)': True, 'has(Hummer)': True, 'has(on)': True, 'has(HIGHER)': True, 'has(wheels)': True, 'has(than)': True, 'has(neccesssary)': True, 'has(OMG)': True, 'has(how)': True, 'has(are)': True, 'has(you)': True, 'has(Home)': True, 'has(Skillit)': True, 'has(?)': True}, 1), ({'has(Ugh)': True, 'has(,)': True, 'has(I)': True, 'has(need)': True, 'has(to)': True, 'has(get)': True, 'has(new)': True, 'has(glasses)': True, 'has(.)': True, 'has(This)': True, 'has(pair)': True, 'has(is)': True, 'has(scratched)': True, 'has(up)': True, 'has(high)': True, 'has(heaven)': True}, 0), ({'has(Happy)': True, 'has(birthday)': True, 'has(me)': True, 'has(!)': True, 'has(evil)': True, 'has(identical)': True, 'has(twin)': True, 'has(We)': True, \"has('re)\": True, 'has(old)': True}, 0), ({'has(some)': True, 'has(one)': True, 'has(buy)': True, 'has(me)': True, 'has(a)': True, 'has(teddy)': True, 'has(bear)': True, 'has(!)': True}, 1), ({'has(Beer)': True, 'has(pong)': True, 'has(with)': True, 'has(@)': True, 'has(ChrisMallin)': True, 'has(,)': True, 'has(Marcie)': True, 'has(an)': True, 'has(AJ)': True, 'has(.)': True, 'has(Minus)': True, 'has(the)': True, 'has(two)': True, 'has(ppl)': True, 'has(I)': True, 'has(made)': True, 'has(up)': True, 'has(to)': True, 'has(seem)': True, 'has(less)': True, 'has(pathetic)': True, 'has(...)': True}, 1)]\n"
     ]
    }
   ],
   "source": [
    "# extract the features from the respective data sets\n",
    "train_features = [(nltk_one_hot_extr(x),y) for (x,y) in train_tweets]\n",
    "test_features = [(nltk_one_hot_extr(x),y) for (x,y) in test_tweets]\n",
    "print(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a Naive Bayes classifier\n",
    "nltk_naive_bayes_classifier = nltk.NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7378"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report the accuracy using the test set\n",
    "nltk.classify.accuracy(nltk_naive_bayes_classifier,test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding of most informative features\n",
    "Use NLTK to build another Naive Bayes classifier that uses the 2000 most informative features and train it on the training set, then report the accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('has(horrible)', True), ('has(terrible)', True), ('has(vip)', True), ('has(hurts)', True), ('has(throat)', True), ('has(pleasure)', True), ('has(surgery)', True), ('has(depressing)', True), ('has(upset)', True), ('has(sad)', True)]\n"
     ]
    }
   ],
   "source": [
    "# obtain the 2000 most informative features\n",
    "most_informative_features = nltk_naive_bayes_classifier.most_informative_features(2000)\n",
    "print(most_informative_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horrible', 'terrible', 'vip', 'hurts', 'throat', 'pleasure', 'surgery', 'depressing', 'upset', 'sad']\n"
     ]
    }
   ],
   "source": [
    "# get the most informative words\n",
    "most_informative_words = [w[4:-1] for w,f in most_informative_features]\n",
    "print(most_informative_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'has(Taylor)': True}, 0), ({}, 1), ({'has(Get)': True}, 1), ({'has(sorry)': True, 'has(left)': True}, 0), ({}, 0), ({}, 1), ({'has(Ugh)': True}, 0), ({'has(Happy)': True}, 0), ({}, 1), ({}, 1)]\n"
     ]
    }
   ],
   "source": [
    "# extract the most informative words from the respective data sets\n",
    "# using the function defined above for extracting features that use one-hot encoding\n",
    "train_features_informative = [(nltk_one_hot_extr(x, most_informative_words),y) for (x,y) in train_tweets]\n",
    "test_features_informative = [(nltk_one_hot_extr(x, most_informative_words),y) for (x,y) in test_tweets]\n",
    "print(train_features_informative[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a Naive Bayes classifier using the list of most informative words\n",
    "nltk_naive_bayes_classifier2 = nltk.NaiveBayesClassifier.train(train_features_informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# report the accuracy of the data of the 2000 most informative features\n",
    "nltk.classify.accuracy(nltk_naive_bayes_classifier2,test_features_informative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf for Naive Bayes in Scikit-Learn\n",
    "Using Scikit-Learn, generate the tf.idf matrix of the training set. With this matrix, train an `sklearn` Naive Bayes classifier using the training set and report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@shabooty : Taylor Swift is 19. She's already way behind on her career. Portman did it better, this just makes me cringe \", '@raccoon9ta you can do it ', \"@TheStitchWitch Not on a friday! Get out the excedrin and caffeine and show that b1+ch who's boss! \", '@LauraKim123 yes, sorry! Saturday ... haha, was obviously very hopeful about the number of days left in this week ', 'cause mom got up late ', '@krystlerb Leave it to u to crack me up! IS that a Hummer on HIGHER wheels than neccesssary! OMG!   how are you Home Skillit?', 'Ugh, I need to get new glasses. This pair is scratched up to high heaven. ', \"Happy birthday me!  Happy birthday evil identical twin! We're old \", 'some one buy me a teddy bear! ', 'Beer pong with @ChrisMallin, Marcie, an AJ. Minus the two ppl I made up to seem less pathetic... ']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(lowercase=False)\n",
    "sklearn_tfidf = tfidf.fit_transform([tweet[0] for tweet in train_tweets])\n",
    "print([tweet[0] for tweet in train_tweets][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Kernel seems to crash when running this code block. Might need more powerful machine or more efficient code...\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "sklearn_tfidf_NB = MultinomialNB()\n",
    "sklearn_tfidf_NB.fit(sklearn_tfidf, [tweet[0] for tweet in train_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "sklearn_tfidf_test = tfidf.transform([tweet[0] for tweet in test_tweets])\n",
    "predictions = sklearn_tfidf_NB.predict(sklearn_tfidf_test)\n",
    "accuracy_score([tweet[0] for tweet in test_tweets], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
